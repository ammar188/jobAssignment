{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Reading in words from\n",
    "#The Notebooks of Leonardo Da Vinci\n",
    "leonardo_words = word_tokenize(open('5000-8.txt', 'r', encoding = \"ISO-8859-1\").read())\n",
    "\n",
    "#The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson\n",
    "arthur_words   = word_tokenize(open('20417.txt', 'r').read())\n",
    "\n",
    "#Ulysses by James Joyce\n",
    "james_words    = word_tokenize(open('4300-0.txt', 'r').read())\n",
    "\n",
    "#The Picture of Dorian Gray by Oscar Wilde\n",
    "oscar_words    = word_tokenize(open('174.txt', 'r').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing single characters and everything else but alphabets from strings and\n",
    "#turning strings to lower cases\n",
    "import re\n",
    "alphatize = lambda x: re.sub('[^a-zA-Z]+', '', x)\n",
    "words1 = [alphatize(x).lower() for x in leonardo_words if alphatize(x) != \"\" and len(alphatize(x)) != 1]\n",
    "words2 = [alphatize(x).lower() for x in arthur_words   if alphatize(x) != \"\" and len(alphatize(x)) != 1]\n",
    "words3 = [alphatize(x).lower() for x in james_words    if alphatize(x) != \"\" and len(alphatize(x)) != 1]\n",
    "words4 = [alphatize(x).lower() for x in oscar_words    if alphatize(x) != \"\" and len(alphatize(x)) != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose to use predefined list from python nltk plus added some garbage words from the book\n",
    "stop_words = [\"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"done\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words from the list of words\n",
    "no_stop_words1 = [word for word in words1 if word not in stop_words]\n",
    "no_stop_words2 = [word for word in words2 if word not in stop_words]\n",
    "no_stop_words3 = [word for word in words3 if word not in stop_words]\n",
    "no_stop_words4 = [word for word in words4 if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose to lemmetize as lemmetizing does not only cut off the end part but reduces to base form\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words1 = [lemmatizer.lemmatize(x) for x in no_stop_words1]\n",
    "lemmatized_words2 = [lemmatizer.lemmatize(x) for x in no_stop_words2]\n",
    "lemmatized_words3 = [lemmatizer.lemmatize(x) for x in no_stop_words3]\n",
    "lemmatized_words4 = [lemmatizer.lemmatize(x) for x in no_stop_words4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words1 = [stemmer.stem(x) for x in lemmatized_words1]\n",
    "stemmed_words2 = [stemmer.stem(x) for x in lemmatized_words2]\n",
    "stemmed_words3 = [stemmer.stem(x) for x in lemmatized_words3]\n",
    "stemmed_words4 = [stemmer.stem(x) for x in lemmatized_words4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def speech(x):\n",
    "    if(wn.synsets(x) == []):\n",
    "        return \"none\"\n",
    "    elif(wn.synsets(x)[0].pos() == \"n\"):\n",
    "        return \"noun\"\n",
    "    elif(wn.synsets(x)[0].pos() == \"v\"):\n",
    "        return \"verb\"\n",
    "    else:\n",
    "        return \"none\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "cnx = mysql.connector.connect(user='root', password='root',\n",
    "                              host='127.0.0.1',\n",
    "                              database='connectavo')\n",
    "cursor = cnx.cursor()\n",
    "cursor.execute(\"CREATE TABLE books (book_ID int NOT NULL PRIMARY KEY,name VARCHAR(255))\")\n",
    "sql = \"INSERT INTO books (book_ID, name) VALUES (%s, %s)\"\n",
    "val = [(0,\"The Notebooks of Leonardo Da Vinci\"),\n",
    "      (1,\"The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson\"),\n",
    "      (2,\"Ulysses by James Joyce\"),\n",
    "      (3,\"The Picture of Dorian Gray by Oscar Wilde\")]\n",
    "cursor.executemany(sql,val)\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE TABLE words (word_ID int NOT NULL PRIMARY KEY, word VARCHAR(255), type VARCHAR(255), book_ID int ,FOREIGN KEY (book_ID) REFERENCES books(book_ID))\")\n",
    "sql = \"INSERT INTO words (word_ID, word, type, book_ID) VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "word_ID = list(range(len(lemmatized_words1)+len(lemmatized_words2)+\n",
    "                len(lemmatized_words3)+len(lemmatized_words4)))\n",
    "words = lemmatized_words1+lemmatized_words2+lemmatized_words3+lemmatized_words4\n",
    "types = [speech(x) for x in words]\n",
    "words = stemmed_words1+stemmed_words2+stemmed_words3+stemmed_words4\n",
    "book_ID = [0] * len(lemmatized_words1) + [1] * len(lemmatized_words2) + [2] * len(lemmatized_words3) + [3] * len(lemmatized_words4)\n",
    "vals = zip(word_ID,words,types,book_ID)\n",
    "for val in vals:\n",
    "    cursor.execute(sql,val)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([lemmatized_words1,lemmatized_words2,lemmatized_words3,lemmatized_words4])\n",
    "corpus = [dictionary.doc2bow(lemmatized_words1),dictionary.doc2bow(lemmatized_words2),\n",
    "          dictionary.doc2bow(lemmatized_words3),dictionary.doc2bow(lemmatized_words4)]\n",
    "\n",
    "tf_idf = TfidfModel(corpus)\n",
    "\n",
    "\n",
    "sims = gensim.similarities.MatrixSimilarity(tf_idf[corpus])\n",
    "\n",
    "query_doc_bow1 = dictionary.doc2bow(lemmatized_words1)\n",
    "query_doc_bow2 = dictionary.doc2bow(lemmatized_words2)\n",
    "query_doc_bow3 = dictionary.doc2bow(lemmatized_words3)\n",
    "query_doc_bow4 = dictionary.doc2bow(lemmatized_words4)\n",
    "query_doc_tf_idf1 = tf_idf[query_doc_bow1]\n",
    "query_doc_tf_idf2 = tf_idf[query_doc_bow2]\n",
    "query_doc_tf_idf3 = tf_idf[query_doc_bow3]\n",
    "query_doc_tf_idf4 = tf_idf[query_doc_bow4]\n",
    "similarity1 = sims[query_doc_tf_idf1]\n",
    "similarity2 = sims[query_doc_tf_idf2]\n",
    "similarity3 = sims[query_doc_tf_idf3]\n",
    "similarity4 = sims[query_doc_tf_idf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE TABLE similarities (book1_ID int, book2_ID int, similarity float(7,4), FOREIGN KEY (book1_ID) REFERENCES books(book_ID), FOREIGN KEY (book2_ID) REFERENCES books(book_ID))\")\n",
    "\n",
    "\n",
    "sql = \"INSERT INTO similarities (book1_ID, book2_ID, similarity) VALUES (%s, %s, %s)\"\n",
    "vals = [(0,0,similarity1[0]*100),(0,1,similarity1[1]*100),(0,2,similarity1[2]*100),(0,3,similarity1[3]*100),\n",
    "        (1,0,similarity2[0]*100),(1,1,similarity2[1]*100),(1,2,similarity2[2]*100),(1,3,similarity2[3]*100),\n",
    "        (2,0,similarity3[0]*100),(2,1,similarity3[1]*100),(2,2,similarity3[2]*100),(2,3,similarity3[3]*100),\n",
    "        (3,0,similarity4[0]*100),(3,1,similarity4[1]*100),(3,2,similarity4[2]*100),(3,3,similarity4[3]*100)\n",
    "       ]\n",
    "\n",
    "cursor.executemany(sql,vals)\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
